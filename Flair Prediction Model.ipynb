{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flair Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer\n",
    "import praw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the data from praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit=praw.Reddit(client_id='Xizrpkw0yLJdBQ',client_secret='IPVu1oXIDtd2jjX8S4PJ499E6vk',user_agent='my_reddit_scraper')\n",
    "subreddit=reddit.subreddit('india')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(post_id_str,topics_dict):\n",
    "    post_id_list=post_id_str.split()\n",
    "    for post_id in post_id_list:\n",
    "        print(\"getting next post id\")\n",
    "        submission=reddit.submission(id=post_id)\n",
    "        topics_dict[\"flair\"].append(str(submission.link_flair_text))\n",
    "        topics_dict[\"title\"].append(submission.title)\n",
    "        topics_dict[\"id\"].append(submission.id)\n",
    "        topics_dict[\"url\"].append(submission.url)\n",
    "        submission.comments.replace_more(limit=None)\n",
    "        comment = ''\n",
    "        numcomments=0\n",
    "        for top_level_comment in submission.comments:\n",
    "            comment = comment + ' ' + top_level_comment.body\n",
    "            numcomments+=1\n",
    "            if numcomments>60:\n",
    "                break\n",
    "        if(len(comment)==0):\n",
    "            topics_dict['comments'].append(\"No comments\")\n",
    "        else:\n",
    "            topics_dict[\"comments\"].append(comment)\n",
    "    print(\"data acquired\")\n",
    "    return topics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def string_form(value):\n",
    "    return str(value)\n",
    "\n",
    "def clean_text(text):\n",
    "   \n",
    "    text = BeautifulSoup(text, \"lxml\").text\n",
    "    text = text.lower()\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "    text = BAD_SYMBOLS_RE.sub('', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=stopwords.words('english')\n",
    "lancaster=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(words): \n",
    "    words=str(words)\n",
    "    if(len(words)==0):\n",
    "        return [lancaster.stem(word) for word in \"No Body\"]\n",
    "    words= BeautifulSoup(words, \"lxml\").text\n",
    "    words=words.lower()\n",
    "    nopunc=[char for char in words if char not in string.punctuation]\n",
    "    nopunc=''.join(nopunc)\n",
    "    wordlist= [lancaster.stem(word) for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "    wordlist=' '.join(wordlist)\n",
    "    return wordlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URL processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_process(words):\n",
    "    words=str(words)\n",
    "    if(len(words)==0):\n",
    "        return [word for word in \"No Body\"]\n",
    "    words= BeautifulSoup(words, \"lxml\").text\n",
    "    words=words.lower()\n",
    "    nopunc=[]\n",
    "    for char in words:\n",
    "        if char in string.punctuation:\n",
    "            nopunc.append(\" \")\n",
    "        else:\n",
    "            if(char.isdigit()==False):\n",
    "                nopunc.append(char)\n",
    "    nopunc=''.join(nopunc)\n",
    "    stop_words=stopwords.words('english')\n",
    "    stop_words.append('comments')\n",
    "    stop_words.append('reddit')\n",
    "    stop_words.append('https')\n",
    "    stop_words.append('r')\n",
    "    stop_words.append('www')\n",
    "    stop_words.extend(['comments','reddit','https','r','www','com','india','http','html','news'])\n",
    "    wordlist= [word for word in nopunc.split() if word.lower() not in stop_words]\n",
    "    wordlist=' '.join(wordlist)\n",
    "    return wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process2(words):\n",
    "    words=str(words)\n",
    "    nopunc=[]\n",
    "    for char in words:\n",
    "        if char in string.punctuation:\n",
    "            nopunc.append(\" \")\n",
    "        else:\n",
    "            if(char.isdigit()==False):\n",
    "                nopunc.append(char)\n",
    "    nopunc=''.join(nopunc)\n",
    "    stop_words=stopwords.words('english')\n",
    "    extended_stopwords=['ind','lik','peopl','ev','com','govern','think','bank','tim','work','dont','fuck','new','mak','said','year','nee','want','country','day','giv','thing','good','say','tak','ind','india','peopl']\n",
    "    stop_words.extend(extended_stopwords)\n",
    "    wordlist= [word for word in nopunc.split() if word.lower() not in stop_words]\n",
    "    wordlist=' '.join(wordlist)\n",
    "    return wordlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='trained_pipeline_pickle.sav'\n",
    "model=pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(post_id):\n",
    "    topics_dict={\"flair\":[], \"title\":[],\"id\":[], \n",
    "                          \"url\":[],\"comments\":[]}        \n",
    "    #get the data\n",
    "    print(\"getting the data\")\n",
    "    data_dict=get_data(post_id,topics_dict)\n",
    "    print(\"converting to dataframe\")\n",
    "    topics_data=pd.DataFrame(data_dict)\n",
    "    # Do data cleaning and stemming to get text data in the processed form\n",
    "    print(\"cleaning the data\")\n",
    "    topics_data['stem_comments']=topics_data['comments'].apply(lambda x: text_process(x))\n",
    "    topics_data['stemmed_titles']=topics_data['title'].apply(lambda x: text_process(x))\n",
    "    topics_data['stemmed_url']=topics_data['url'].apply(lambda x: url_process(x))\n",
    "    topics_data['title_comments_stem']=topics_data['stemmed_titles']+topics_data['stem_comments']\n",
    "    #Get the prediction\n",
    "    print(\"using model to predict flairs\")\n",
    "    flair_text=topics_data['flair']\n",
    "    prediction=model.predict(topics_data['title_comments_stem']+topics_data['stemmed_url'])\n",
    "    print(\"Model prediction is \",prediction[0], \"\\n Original post flair is \",topics_data['flair'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting the data\n",
      "getting next post id\n",
      "data acquired\n",
      "converting to dataframe\n",
      "cleaning the data\n",
      "using model to predict flairs\n",
      "Model prediction is  AskIndia \n",
      " Original post flair is  Series([], Name: flair, dtype: object)\n"
     ]
    }
   ],
   "source": [
    "predict('7nc7wa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction is  AskIndia \n",
      " Original post flair is  0    AskIndia\n",
      "Name: flair, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mihee\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:389: UserWarning: \"https://www.reddit.com/r/india/comments/g3jeid/voicemail_service_in_india/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "predict('g3jeid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction is  Politics \n",
      " Original post flair is  0    Politics\n",
      "Name: flair, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mihee\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:389: UserWarning: \"https://i.redd.it/om0pk80lvr441.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "predict('eaxdzc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction is  Politics \n",
      " Original post flair is  Series([], Name: flair, dtype: object)\n"
     ]
    }
   ],
   "source": [
    "predict('g41sd9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
